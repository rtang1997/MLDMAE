{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_type=\"sprial\"\n",
        "penalty=\"MMD\"\n",
        "#penalty=\"WASS\"\n",
        "center=10\n",
        "latent_dim=1\n",
        "intermediate_dim=128\n",
        "input_dim=2\n",
        "\n",
        "# data_type=\"torus\"\n",
        "# center=15\n",
        "# lanten_dim=2\n",
        "# input_dim=3\n"
      ],
      "metadata": {
        "id": "dvoKd2F964oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e34xHT6A9VBJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc623c98-68b4-4c35-8b48-d01d5abb51ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting POT\n",
            "  Downloading POT-0.8.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (664 kB)\n",
            "\u001b[K     |████████████████████████████████| 664 kB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from POT) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from POT) (1.7.3)\n",
            "Installing collected packages: POT\n",
            "Successfully installed POT-0.8.2\n"
          ]
        }
      ],
      "source": [
        "!pip install POT\n",
        "import ot\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import math, os\n",
        "from numpy import array as matrix, arange\n",
        "from numpy import zeros, ones,empty\n",
        "import tensorflow_datasets\n",
        "import random\n",
        "import math\n",
        "import sklearn\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras import backend as K\n",
        "from matplotlib import pyplot as plt\n",
        "import math, os\n",
        "from numpy import array as matrix, arange\n",
        "from numpy import zeros, ones,empty\n",
        "import tensorflow_datasets\n",
        "import random\n",
        "import math\n",
        "import sklearn\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from tensorflow.keras import datasets, layers, models,constraints\n",
        "from scipy.stats import truncnorm "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDlsbK4V9WP7"
      },
      "outputs": [],
      "source": [
        "#generate training data\n",
        "import matplotlib.pyplot as plt\n",
        "if data_type==\"sprial\":\n",
        "  r= 1\n",
        "  phi = 3 * np.pi * np.random.rand(1000)\n",
        "  x = r * np.cos(phi+2)*phi/(np.pi)\n",
        "  y = r * np.sin(phi+2)*phi*2/(np.pi)\n",
        "  D1=np.array([x,y]).T\n",
        "  x_train=D1\n",
        "if data_type==\"torus\":\n",
        "  R= 3\n",
        "  r=1\n",
        "  data=[]\n",
        "  n=3000\n",
        "  o=0\n",
        "  iend=0\n",
        "  phi = 2* np.pi * np.random.rand(n)\n",
        "  theta=2* np.pi * np.random.rand(n)\n",
        "  x=(R+r*np.cos(theta))*np.cos(phi)\n",
        "  y=(R+r*np.cos(theta))*np.sin(phi)\n",
        "  z=r*np.sin(theta)\n",
        "  D1=np.array([x,y,z]).T\n",
        "  x_train=D1\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-ZegAxW-jaW"
      },
      "outputs": [],
      "source": [
        "##find partition of unity\n",
        "\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "kmeans = sklearn.cluster.MiniBatchKMeans(n_clusters=center,batch_size=1000)\n",
        "kmeans = kmeans.fit(x_train)\n",
        "A=kmeans.cluster_centers_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DD4XX-atzhLh"
      },
      "outputs": [],
      "source": [
        "nn=x_train.shape[0]\n",
        "gg=zeros((center,nn))\n",
        "for i in range(nn):\n",
        "  for j in range(center):\n",
        "    gg[j,i]=np.dot(np.transpose(x_train[i,:]-A[j,:]),(x_train[i,:]-A[j,:]))\n",
        "label=zeros((nn,1))\n",
        "for i in range(nn):\n",
        "  label[i,0]=np.argmin(gg[:,i])\n",
        "radius=zeros((center,1))\n",
        "for i in range(center):\n",
        "  radius[i,0]=np.max(gg[i,np.where(label[:,0]==i)])**0.5+1\n",
        "rhopar=10\n",
        "def rho(xindex,r,cindex,gg):\n",
        "  z=gg[cindex,xindex]**0.5\n",
        "  if r>=z:\n",
        "    return (r**2-z**2)**rhopar\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def trho(index,xindex,gg):\n",
        " z=0\n",
        " for j in range(center):\n",
        "   z=z+rho(xindex,radius[j,0],j,gg)\n",
        " return rho(xindex,radius[index,0],index,gg)/z\n",
        "prob=zeros((center,1))\n",
        "for j in range(center):\n",
        "  for i in  range(nn):\n",
        "      prob[j,0]=prob[j,0]+trho(j,i,gg)/nn\n",
        "nprob=empty((center,1))\n",
        "for j in range(center):\n",
        "   nprob[j,0]=round(prob[j,0]*100)\n",
        "nprob[np.argmax(nprob)]=nprob[np.argmax(nprob)]+100-sum(nprob)\n",
        "nnprob=empty((center,1))\n",
        "for j in range(center):\n",
        "   nnprob[j,0]=round(prob[j,0]*10000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pourho0(index,x):\n",
        " zz1=tf.expand_dims(x-A[index,:],1)\n",
        " zz2=tf.expand_dims(x-A[index,:],2)\n",
        " zz=tf.squeeze(tf.matmul(zz1,zz2)**0.5,axis=2)\n",
        " return  tf.cast(radius[j,0]>=zz,tf.float32)*(radius[j,0]**2-zz**2)**rhopar\n",
        "def pourho(index,x):\n",
        " z=0\n",
        " for j in range(center):\n",
        "   z=z+pourho0(j,x)\n",
        " return pourho0(index,x)/z"
      ],
      "metadata": {
        "id": "EGRbpvwQ_0tT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hkc6mENc-28F"
      },
      "outputs": [],
      "source": [
        "def compute_kernel(x, y):\n",
        "    x_size = tf.shape(x)[0]\n",
        "    y_size = tf.shape(y)[0]\n",
        "    dim = tf.shape(x)[1]\n",
        "     \n",
        "    tiled_x = tf.tile(tf.reshape(x, tf.stack([x_size, 1, dim])), tf.stack([1, y_size, 1]))\n",
        "    tiled_y = tf.tile(tf.reshape(y, tf.stack([1, y_size, dim])), tf.stack([x_size, 1, 1]))\n",
        "    return tf.exp(-tf.reduce_mean(tf.square(tiled_x - tiled_y), axis=2) / tf.cast(dim, tf.float32))\n",
        "def compute_mmd(x, y):\n",
        "    x_kernel = compute_kernel(x, x)\n",
        "    y_kernel = compute_kernel(y, y)\n",
        "    xy_kernel = compute_kernel(x, y)\n",
        "    return tf.reduce_mean(x_kernel)+ tf.reduce_mean(y_kernel) - 2 * tf.reduce_mean(xy_kernel) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRA3nkvO-5Om"
      },
      "outputs": [],
      "source": [
        "if data_type==\"sprial\":\n",
        "  def compute_wass(x,y):\n",
        "    val=0\n",
        "    x1=tf.sort(x,axis=0)\n",
        "    y1=tf.sort(y,axis=0)\n",
        "    \n",
        "    val=val+tf.reduce_mean(tf.abs(x1-y1))\n",
        "    \n",
        "\n",
        "    return val\n",
        "    \n",
        "if data_type==\"torus\":\n",
        "  def compute_wass(x,y):\n",
        "    M=(tf.reduce_sum(tf.abs(tf.expand_dims(x,1) - tf.expand_dims(y,0)), 2))\n",
        "    u1 = ot.utils.unif(x.shape[0])\n",
        "    u2 = ot.utils.unif(y.shape[0])\n",
        "    M1=M.numpy()\n",
        "    w=ot.emd(u1,u2,M1)\n",
        "    val=tf.reduce_sum(w*M)\n",
        "\n",
        "    return val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXwKaDLOSh6k",
        "outputId": "ced94b0f-b4c9-435f-c160-f56872f5b005"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder_cov\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 3)]               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               512       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 512\n",
            "Trainable params: 512\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"encoder_linear\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 128)]             0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 15)                1935      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,935\n",
            "Trainable params: 1,935\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "encoder_inputs = keras.Input(shape=(input_dim))\n",
        "encoder_outputs = layers.Dense(units=intermediate_dim, activation='relu')(encoder_inputs)\n",
        "encoder_cov = keras.Model(encoder_inputs, encoder_outputs, name=\"encoder_cov\")\n",
        "encoder_cov.summary()\n",
        "encoder_linear_inputs=keras.Input(shape=(intermediate_dim,))\n",
        "encoder_linear_outputs = layers.Dense(latent_dim*center)(encoder_linear_inputs)\n",
        "encoder_linear = keras.Model(encoder_linear_inputs, encoder_linear_outputs, name=\"encoder_linear\")\n",
        "encoder_linear.summary()\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOLFVbpdL5bL",
        "outputId": "f8848299-093b-4482-9b1e-f83ca71c4247"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"decoder_linear\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1920)              3840      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,840\n",
            "Trainable params: 3,840\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"decoder_cov\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 128)]             0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 3)                 387       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 387\n",
            "Trainable params: 387\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "  \n",
        "latent_inputs = keras.Input(shape=(latent_dim,))\n",
        "decoder_linear_outputs = layers.Dense(intermediate_dim*center, activation='relu')(latent_inputs)\n",
        "decoder_linear = keras.Model(latent_inputs, decoder_linear_outputs, name=\"decoder_linear\")\n",
        "decoder_linear.summary()\n",
        "decoder_inputs= keras.Input(shape=(intermediate_dim,))\n",
        "\n",
        "decoder_outputs =layers.Dense(input_dim)(decoder_inputs)\n",
        " \n",
        "decoder_cov = keras.Model(decoder_inputs, decoder_outputs, name=\"decoder_cov\")\n",
        "decoder_cov.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryD1dPUG50yb"
      },
      "outputs": [],
      "source": [
        "####prior $\\nu_0$######\n",
        "\n",
        "def samplelatent(n,latent_dim,j=0):\n",
        "    r=3\n",
        "    x=tf.constant(truncnorm.rvs(-r,r,size=(n, latent_dim)),dtype='float32')\n",
        "    return x\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tl7JG9avgHsk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR9LKTnXhRfu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgeb11zsFVCY"
      },
      "outputs": [],
      "source": [
        " \n",
        "class VAE(keras.Model):\n",
        "    def __init__(self, encoder_cov,encoder_linear, decoder_cov,decoder_linear, **kwargs):\n",
        "        super(VAE, self).__init__(**kwargs)\n",
        "        self.encoder_cov = encoder_cov\n",
        "        self.decoder_cov = decoder_cov\n",
        "        self.encoder_linear = encoder_linear\n",
        "        self.decoder_linear = decoder_linear\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.penalty_loss_tracker = keras.metrics.Mean(name=\"penalty_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.penalty_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "          \n",
        "        data1=tf.reshape(data,(batch_size,input_dim))\n",
        "        rhovalue=[]\n",
        "        for i in range(center):\n",
        "          a=tf.sqrt(tf.reduce_sum(tf.square(data1-tf.transpose(tf.repeat(tf.expand_dims(A[i,],-1),data.shape[0],axis=1))),axis=1))\n",
        "          rhovalue.append(tf.math.multiply(tf.cast(tf.where(radius[i,0]>=a, 1, 0),'float32'),(radius[i,0]**2-a**2)**rhopar))\n",
        "        b=tf.reduce_sum(tf.stack(rhovalue),axis=0)\n",
        "        for i in range(center):\n",
        "          rhovalue[i]=rhovalue[i]/b\n",
        "        \n",
        "        x=[]\n",
        "        batch=[]\n",
        "        for centerite in range(center):\n",
        "            kk=tf.where(tf.where(tf.random.uniform([data.shape[0]])<rhovalue[centerite],0,1)==0) \n",
        "            x.append(tf.gather_nd(data, kk))\n",
        "            batch.append(kk.shape[0])\n",
        "      \n",
        "        x=tf.concat(x,axis=0)\n",
        "        with tf.GradientTape() as tape:\n",
        "            meanlist=encoder_linear(encoder_cov(x))\n",
        "            meanlist=tf.split(meanlist,batch,axis=0)\n",
        "            for i in range(center):\n",
        "                    meanlist[i]=meanlist[i][:,(i*latent_dim):((i+1)*latent_dim)]\n",
        "            meanlistconcat=tf.concat(meanlist,axis=0)\n",
        "            predsini=decoder_linear(meanlistconcat)\n",
        "            predsini=tf.split(predsini,batch,axis=0)\n",
        "            for i in range(center):\n",
        "                    predsini[i]=predsini[i][:,(i*intermediate_dim):((i+1)*intermediate_dim)]\n",
        "            predsini=tf.concat(predsini,axis=0)\n",
        "            predslist=decoder_cov(predsini)\n",
        "            x=tf.split(x,batch,axis=0)\n",
        "            predslist=tf.split(predslist,batch,axis=0)\n",
        "            reconstruction_loss=0\n",
        "            for i in range(center):\n",
        "                      reconstruction_loss=reconstruction_loss+center*keras.metrics.mean_squared_error(K.flatten(x[i]), K.flatten(predslist[i]))*prob[i]\n",
        "            penalty_loss=0\n",
        "            for i in range(center):\n",
        "               if penalty==\"MMD\":\n",
        "                    penalty_loss= penalty_loss+10*compute_mmd(samplelatent(meanlist[i].shape[0], latent_dim,i), meanlist[i])\n",
        "               if penalty==\"WASS\":\n",
        "                    penalty_loss= penalty_loss+compute_wass(samplelatent(meanlist[i].shape[0], latent_dim,i), meanlist[i])\n",
        "            total_loss=K.mean(reconstruction_loss + penalty_loss)\n",
        "              \n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.penalty_loss_tracker.update_state(penalty_loss)\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"penalty_loss\": self.penalty_loss_tracker.result(),\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZpfOhKbFhPX"
      },
      "outputs": [],
      "source": [
        " \n",
        " \n",
        "x_train=x_train.astype('float32')\n",
        "label=label\n",
        "A=A.astype('float32')\n",
        " \n",
        "batch_size=200\n",
        "vae = VAE(encoder_cov, encoder_linear,decoder_cov,decoder_linear)\n",
        "vae.compile(optimizer=keras.optimizers.Adam(0.001),run_eagerly=True)\n",
        "vae.fit(x_train,epochs=1000, batch_size=200)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iy_MmbeoAzoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####update prior###\n",
        "\n",
        "import copy\n",
        "decoder_cov1=copy.deepcopy(decoder_cov)\n",
        "decoder_linear1=copy.deepcopy(decoder_linear)\n",
        "####data-driven prior######\n",
        "\n",
        "def samplelatent(n,latent_dim,j):\n",
        "    r=3\n",
        " \n",
        "    iend=0\n",
        "    x=[]\n",
        "    o=0\n",
        "    for i in range(100):\n",
        "     if iend==0:\n",
        "        data=tf.constant(truncnorm.rvs(-r,r,size=(3*n, latent_dim)),dtype='float32')\n",
        "        aaa=(tf.random.uniform(shape=[data.shape[0],latent_dim])<=pourho(j,decoder_cov1(decoder_linear1(data)[:,(j*intermediate_dim):((j+1)*intermediate_dim)])))\n",
        "        ind=(tf.cast(tf.where(aaa==True)[:,0],tf.int32))\n",
        "        data=tf.gather(data,ind)\n",
        "        o=o+ind.shape[0]\n",
        "        x.append(data)\n",
        "        if o>=n:\n",
        "          iend=1\n",
        "    x=tf.concat(x,axis=0)[0:n,:]\n",
        "    \n",
        "    return x\n",
        "###train the model with the the data-driven prior####\n",
        "vae = VAE(encoder_cov, encoder_linear,decoder_cov,decoder_linear)\n",
        "vae.compile(optimizer=keras.optimizers.Adam(0.001),run_eagerly=True)\n",
        "vae.fit(x_train,epochs=1000, batch_size=200)"
      ],
      "metadata": {
        "id": "vZYvo16y9VQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3bJA3WTQE2G"
      },
      "outputs": [],
      "source": [
        "### Compute W_1 distance between generated samples and test samples###\n",
        "r= 1\n",
        "phi = 3 * np.pi * np.random.rand(30000)\n",
        "\n",
        "\n",
        "\n",
        "x = r * np.cos(phi+2)*phi/(np.pi)\n",
        "y = r * np.sin(phi+2)*phi*2/(np.pi)\n",
        "D1=np.array([x,y]).T\n",
        "x_test=D1\n",
        "\n",
        "z_sample=[]\n",
        "for i in range(center):\n",
        "  z_sample.append(samplelatent(int(np.round(nnprob[i]*3)), latent_dim,i))\n",
        "a=decoder_cov(decoder_linear(z_sample[0])[:,(0*intermediate_dim):((0+1)*intermediate_dim)])\n",
        "for i in range(1,center):\n",
        "  a=tf.concat([decoder_cov(decoder_linear(z_sample[i])[:,(i*intermediate_dim):((i+1)*intermediate_dim)]),a],axis=0)\n",
        "a=a.numpy()\n",
        "u1 = ot.utils.unif(a.shape[0])\n",
        "u2 = ot.utils.unif(x_test.shape[0])\n",
        "M=ot.dist(x_test,a,metric='minkowski',p=1)\n",
        "w = ot.emd2(u2, u1, M)\n",
        "print(w)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}