{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpVP_XKzaQJe"
      },
      "outputs": [],
      "source": [
        "#is_copied=0\n",
        "is_copied=1\n",
        "bandwidth=0.01\n",
        "#bandwidth=0\n",
        "batch_size=256\n",
        "#batch_size=128\n",
        "center=5\n",
        "#center=1\n",
        "latent_dim=64\n",
        "pou='indicator'\n",
        "gamma=10\n",
        "#pou='smooth'\n",
        "penalty=\"MMD\"\n",
        "#penalty=\"SWASS\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ey15C-HI5lHk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras import backend as K\n",
        "  \n",
        "  \n",
        "from matplotlib import pyplot as plt\n",
        "import math, os\n",
        "from numpy import array as matrix, arange\n",
        "from numpy import zeros, ones,empty\n",
        "import tensorflow_datasets\n",
        "import random\n",
        "import math\n",
        "import sklearn\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from tensorflow.keras import datasets, layers, models,constraints\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "import gdown\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from natsort import natsorted\n",
        " \n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fe8Hm3NoulL_"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import datasets, layers, models,constraints\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "import gdown\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from natsort import natsorted\n",
        " \n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Y5XzcfEjBzJ"
      },
      "outputs": [],
      "source": [
        "!mkdir celeba && wget https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/celeba.zip \n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"celeba.zip\",\"r\") as zip_ref:\n",
        "  zip_ref.extractall(\"celeba/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "la3WEfwsjIXc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "root = './celeba/img_align_celeba'\n",
        "img_list = os.listdir(root)\n",
        "print(len(img_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MksvuEG0kiPM"
      },
      "outputs": [],
      "source": [
        "class CelebADataset(Dataset):\n",
        "  def __init__(self, root_dir, transform=None):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      root_dir (string): Directory with all the images\n",
        "      transform (callable, optional): transform to be applied to each image sample\n",
        "    \"\"\"\n",
        "    # Read names of images in the root directory\n",
        "    image_names = os.listdir(root_dir)\n",
        "\n",
        "    self.root_dir = root_dir\n",
        "    self.transform = transform \n",
        "    self.image_names = natsorted(image_names)\n",
        "\n",
        "  def __len__(self): \n",
        "    return len(self.image_names)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # Get the path to the image \n",
        "    img_path = os.path.join(self.root_dir, self.image_names[idx])\n",
        "    # Load image and convert it to RGB\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    # Apply transformations to the image\n",
        "    if self.transform:\n",
        "      img = self.transform(img)\n",
        "\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kjth7TH5dV22"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Spatial size of training images, images are resized to this size.\n",
        "image_size = 64\n",
        "# Transformations to be applied to each individual image sample\n",
        "transform=transforms.Compose([\n",
        "   transforms.CenterCrop(140),\n",
        "   transforms.Resize(image_size),\n",
        "  \n",
        "    transforms.ToTensor(),\n",
        " \n",
        "])\n",
        "# Load the dataset from file and apply transformations\n",
        "celeba_dataset = CelebADataset(root, transform)\n",
        "\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0WERlthqYjR"
      },
      "outputs": [],
      "source": [
        "import torchvision.utils as vutils\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgR0svcjqwet"
      },
      "outputs": [],
      "source": [
        "celeba_dataloader = torch.utils.data.DataLoader(celeba_dataset,shuffle=False,batch_size=len(celeba_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMIZcjhmCcHL"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YFAzmyCghAM"
      },
      "outputs": [],
      "source": [
        "X=next(iter(celeba_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OV_nYdZDq1Xt"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "shutil.rmtree('celeba/')\n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvTHlbWWo4Id"
      },
      "outputs": [],
      "source": [
        "X=X.numpy()\n",
        "X=X.transpose([0,2,3,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGjY1-N1xkxm"
      },
      "outputs": [],
      "source": [
        "n_train=180000\n",
        "n_test=X.shape[0]-n_train\n",
        "x_train=X[0:n_train,:,:,:]\n",
        "x_test=X[n_train:X.shape[0],:,:,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UymqyjMzoUop"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZf40QDo_HmD"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "del(X)\n",
        "del(celeba_dataset)\n",
        " \n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Vjp9OE52LVE"
      },
      "outputs": [],
      "source": [
        "x_train=np.reshape(x_train,(x_train.shape[0],64*64*3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8nWZztjx6wR"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import MiniBatchKMeans\n",
        "kmeans = sklearn.cluster.MiniBatchKMeans(n_clusters=center,batch_size=1000)\n",
        "kmeans = kmeans.fit(x_train)\n",
        "A=kmeans.cluster_centers_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-MMoyMy3C9i"
      },
      "outputs": [],
      "source": [
        "nn=x_train.shape[0]\n",
        "gg=zeros((center,nn))\n",
        "for i in range(nn):\n",
        "  for j in range(center):\n",
        "    gg[j,i]=np.dot(np.transpose(x_train[i,:]-A[j,:]),(x_train[i,:]-A[j,:]))\n",
        "label=zeros((nn,1))\n",
        "for i in range(nn):\n",
        "  label[i,0]=np.argmin(gg[:,i])\n",
        "\n",
        "radius=zeros((center,1))\n",
        "for i in range(center):\n",
        "  radius[i,0]=np.max(gg[i,np.where(label[:,0]==i)])**0.5+1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rk5pPpmf73N-"
      },
      "outputs": [],
      "source": [
        " \n",
        "if pou=='smooth':\n",
        "  def rho(xindex,r,cindex,gg):\n",
        "    z=gg[cindex,xindex]**0.5\n",
        "    if r>=z:\n",
        "      return (r**2-z**2)**gamma\n",
        "    else:\n",
        "      return 0\n",
        "\n",
        "  def trho(index,xindex,gg):\n",
        "    z=0\n",
        "    for j in range(center):\n",
        "      z=z+rho(xindex,radius[j,0],j,gg)\n",
        "    return rho(xindex,radius[index,0],index,gg)/z\n",
        "  prob=zeros((center,1))\n",
        "  for j in range(center):\n",
        "    for i in  range(nn):\n",
        "      prob[j,0]=prob[j,0]+trho(j,i,gg)/nn     \n",
        "\n",
        "if pou=='indicator':\n",
        "   prob=zeros((center,1))\n",
        "   for j in range(center):\n",
        "        prob[j,0]=len(np.where(label==j)[0])/nn\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qc4AkHTz9QZY"
      },
      "outputs": [],
      "source": [
        "nprob=empty((center,1))\n",
        "for j in range(center):\n",
        "   nprob[j,0]=round(prob[j,0]*100)\n",
        "nprob[np.argmax(nprob)]=nprob[np.argmax(nprob)]+100-sum(nprob)\n",
        "nnprob=empty((center,1))\n",
        "for j in range(center):\n",
        "   nnprob[j,0]=round(prob[j,0]*10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veZC9sdgxf1E"
      },
      "outputs": [],
      "source": [
        "x_train=np.reshape(x_train,(x_train.shape[0],64,64,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMAy22sTBa3G"
      },
      "outputs": [],
      "source": [
        "latent_dim = 64\n",
        "intermediate_dim=8*8*1024\n",
        " \n",
        "input_dim=64\n",
        "n_channel=3\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4TfotgKzkTc"
      },
      "outputs": [],
      "source": [
        "#RBF kernel\n",
        "# def compute_kernel(x, y):\n",
        "#     x_size = tf.shape(x)[0]\n",
        "#     y_size = tf.shape(y)[0]\n",
        "#     dim = tf.shape(x)[1]\n",
        "#     depth = tf.shape(x)[2]\n",
        "#     tiled_x = tf.tile(tf.reshape(x, tf.stack([x_size, 1, dim,depth])), tf.stack([1, y_size, 1,1]))\n",
        "#     tiled_y = tf.tile(tf.reshape(y, tf.stack([1, y_size, dim,depth])), tf.stack([x_size, 1, 1,1]))\n",
        "#     return tf.exp(-tf.reduce_sum(tf.square(tiled_x - tiled_y), axis=2) / tf.cast(dim, tf.float32))\n",
        "  \n",
        "#IMQ kernel\n",
        "def compute_kernel(x, y,h):\n",
        "    x_size = tf.shape(x)[0]\n",
        "    y_size = tf.shape(y)[0]\n",
        "    dim = tf.shape(x)[1]\n",
        "   \n",
        "    tiled_x = tf.tile(tf.reshape(x, tf.stack([x_size, 1, dim])), tf.stack([1, y_size, 1,]))\n",
        "    tiled_y = tf.tile(tf.reshape(y, tf.stack([1, y_size, dim])), tf.stack([x_size, 1, 1]))\n",
        "    return 2*h*tf.cast(dim, tf.float32)/(2*h*tf.cast(dim, tf.float32)+tf.reduce_sum(tf.square(tiled_x - tiled_y), axis=2))\n",
        "\n",
        "\n",
        "def compute_mmd(x, y):\n",
        "    stat=0\n",
        "    x_size = tf.shape(x)[0]\n",
        "    y_size = tf.shape(y)[0]\n",
        "    for scale in [1]:\n",
        "        h=scale\n",
        "        x_kernel = compute_kernel(x, x,h)-tf.eye(x_size)\n",
        "        y_kernel = compute_kernel(y, y,h)-tf.eye(y_size)\n",
        "        xy_kernel = compute_kernel(x, y,h)\n",
        "        stat=stat+ tf.cast(x_size/(x_size-1),'float32')*tf.reduce_mean(x_kernel)  + tf.cast(y_size/(y_size-1),'float32')*tf.reduce_mean(y_kernel)  - 2 * tf.reduce_mean(xy_kernel)\n",
        "    return stat\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sample_spherical(npoints, ndim):\n",
        "    vec = np.random.randn(ndim, npoints)\n",
        "    vec /= np.linalg.norm(vec, axis=0)\n",
        "    return vec"
      ],
      "metadata": {
        "id": "ohLrqy2ps6G6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_swass(x,y):\n",
        "  val=0\n",
        "  direction=sample_spherical(30,latent_dim)\n",
        "  x=tf.matmul(x,direction)\n",
        "  y=tf.matmul(y,direction)\n",
        "  x1=tf.sort(x,axis=0)\n",
        "  y1=tf.sort(y,axis=0)\n",
        "  \n",
        "  val=val+tf.reduce_mean(tf.abs(x1-y1))\n",
        "  #val=val+keras.metrics.mean_squared_error(K.flatten(x1),K.flatten(y1))\n",
        "\n",
        "  return val"
      ],
      "metadata": {
        "id": "zNws1jqIqsiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ji6ej81ifWeK"
      },
      "outputs": [],
      "source": [
        "\n",
        "encoder_inputs = keras.Input(shape=(64, 64, 3))\n",
        "x=layers.Conv2D(128, 5,strides=2,padding=\"same\")(encoder_inputs)\n",
        "x= layers.BatchNormalization()(x)\n",
        "x = layers.ReLU()(x)\n",
        "x = layers.Conv2D(256, 5,strides=2,padding=\"same\")(x)\n",
        "x= layers.BatchNormalization()(x)\n",
        "x = layers.ReLU()(x)\n",
        "x = layers.Conv2D(512, 5,strides=2,padding=\"same\")(x)\n",
        "x= layers.BatchNormalization()(x)\n",
        "x = layers.ReLU()(x)\n",
        "x = layers.Conv2D(1024, 5,strides=2,padding=\"same\")(x)\n",
        "x= layers.BatchNormalization()(x)\n",
        "x = layers.ReLU()(x)\n",
        "encoder_outputs = layers.Flatten()(x)\n",
        " \n",
        " \n",
        "encoder_cov = keras.Model(encoder_inputs, encoder_outputs, name=\"encoder_cov\")\n",
        "encoder_cov.summary()\n",
        "\n",
        "\n",
        "\n",
        "encoder_linear_inputs=keras.Input(shape=(16384,))\n",
        "encoder_linear_outputs = layers.Dense(latent_dim*center)(encoder_linear_inputs)\n",
        "encoder_linear = keras.Model(encoder_linear_inputs, encoder_linear_outputs, name=\"encoder_linear\")\n",
        "encoder_linear.summary()\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSYf-NYHwwYR"
      },
      "outputs": [],
      "source": [
        "latent_inputs = keras.Input(shape=(latent_dim,))\n",
        "decoder_linear_outputs = layers.Dense(intermediate_dim*center, activation=\"relu\")(latent_inputs)\n",
        "decoder_linear = keras.Model(latent_inputs, decoder_linear_outputs, name=\"decoder_linear\")\n",
        "decoder_linear.summary()\n",
        "decoder_inputs= keras.Input(shape=(intermediate_dim,))\n",
        "x = layers.Reshape((8, 8, 1024))(decoder_inputs)\n",
        "x = layers.Conv2DTranspose(512, 5, strides=2, padding=\"same\")(x)\n",
        "x= layers.BatchNormalization()(x)\n",
        "x = layers.ReLU()(x)\n",
        "x = layers.Conv2DTranspose(256, 5, strides=2, padding=\"same\")(x)\n",
        "x= layers.BatchNormalization()(x)\n",
        "x = layers.ReLU()(x)\n",
        "x = layers.Conv2DTranspose(128, 5, strides=2, padding=\"same\")(x)\n",
        "x= layers.BatchNormalization()(x)\n",
        "x = layers.ReLU()(x)\n",
        "decoder_outputs = layers.Conv2DTranspose(3, 3, strides=1,activation=\"tanh\", padding=\"same\")(x)\n",
        "decoder_cov = keras.Model(decoder_inputs, decoder_outputs, name=\"decoder_cov\")\n",
        "decoder_cov.summary()\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOy3HBbkxzo2"
      },
      "outputs": [],
      "source": [
        "\n",
        "  \n",
        "class VAE(keras.Model):\n",
        "    def __init__(self, encoder_cov,encoder_linear, decoder_cov,decoder_linear, **kwargs):\n",
        "        super(VAE, self).__init__(**kwargs)\n",
        "        self.encoder_cov = encoder_cov\n",
        "        self.decoder_cov = decoder_cov\n",
        "        self.encoder_linear = encoder_linear\n",
        "        self.decoder_linear = decoder_linear\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.mmd_loss_tracker = keras.metrics.Mean(name=\"mmd_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.mmd_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        if pou=='smooth':\n",
        "          data1=tf.reshape(data,(batch_size,64*64*3))\n",
        "          rhovalue=[]\n",
        "          for i in range(center):\n",
        "            a=tf.sqrt(tf.reduce_sum(tf.square(data1-tf.transpose(tf.repeat(tf.expand_dims(A[i,],-1),data.shape[0],axis=1))),axis=1))\n",
        "            rhovalue.append(tf.math.multiply(tf.cast(tf.where(radius[i,0]>=a, 1, 0),'float32'),(radius[i,0]**2-a**2)**gamma))\n",
        "          b=tf.reduce_sum(tf.stack(rhovalue),axis=0)\n",
        "          for i in range(center):\n",
        "            rhovalue[i]=rhovalue[i]/b\n",
        "          x=[]\n",
        "          batch=[]\n",
        "          for centerite in range(center):\n",
        "              kk=tf.where(tf.where(tf.random.uniform([data.shape[0]])<rhovalue[centerite],0,1)==0) \n",
        "              x.append(tf.gather_nd(data, kk))\n",
        "              batch.append(kk.shape[0])\n",
        "      \n",
        "        if pou=='indicator':\n",
        "          data1,label=data\n",
        "          x=[]\n",
        "          batch=[]\n",
        "          for centerite in range(center):\n",
        "              kk=tf.where(label[:,0]==centerite)\n",
        "              x.append(tf.gather_nd(data1, kk))\n",
        "              batch.append(kk.shape[0])\n",
        "      \n",
        "        x=tf.concat(x,axis=0)\n",
        "        with tf.GradientTape() as tape:\n",
        "            meanlist=encoder_linear(encoder_cov(x))\n",
        "            meanlist=tf.split(meanlist,batch,axis=0)\n",
        "            for i in range(center):\n",
        "                    meanlist[i]=meanlist[i][:,(i*latent_dim):((i+1)*latent_dim)]\n",
        "            meanlistconcat=tf.concat(meanlist,axis=0)\n",
        "            predsini=decoder_linear(meanlistconcat)\n",
        "            predsini=tf.split(predsini,batch,axis=0)\n",
        "            for i in range(center):\n",
        "                    predsini[i]=predsini[i][:,(i*intermediate_dim):((i+1)*intermediate_dim)]\n",
        "            predsini=tf.concat(predsini,axis=0)\n",
        "            predslist=decoder_cov(predsini)\n",
        "            x=tf.split(x,batch,axis=0)\n",
        "            predslist=tf.split(predslist,batch,axis=0)\n",
        "            reconstruction_loss=0\n",
        "            for i in range(center):\n",
        "                      reconstruction_loss=reconstruction_loss+input_dim * input_dim * keras.metrics.mean_squared_error(K.flatten(x[i]), K.flatten(predslist[i]))*prob[i]\n",
        "            mmd_loss=0\n",
        "            for i in range(center):\n",
        "              if is_copied==1:\n",
        "                  meanlist[i]=tf.concat([meanlist[i],meanlist[i]],axis=0)\n",
        "             \n",
        "              add_meanlist=bandwidth*tf.random.normal(shape =[meanlist[i].shape[0], latent_dim])\n",
        "              meanlist[i]=meanlist[i]+ add_meanlist\n",
        "              if penalty==\"MMD\":\n",
        "                  mmd_loss= mmd_loss+100/center*compute_mmd(tf.random.normal(shape = [meanlist[i].shape[0], latent_dim]), meanlist[i])\n",
        "              if penalty==\"SWASS\":\n",
        "                  mmd_loss= mmd_loss+30/center*compute_swass(tf.random.normal(shape = [meanlist[i].shape[0], latent_dim]), meanlist[i])\n",
        "             \n",
        "            \n",
        "            total_loss=K.mean(reconstruction_loss + mmd_loss)\n",
        "              \n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.mmd_loss_tracker.update_state(mmd_loss)\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"mmd_loss\": self.mmd_loss_tracker.result(),\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guk-e0DZ2lw3"
      },
      "outputs": [],
      "source": [
        "x_train=x_train.reshape(x_train.shape[0],input_dim,input_dim,n_channel)\n",
        "    \n",
        "x_train=x_train[0:179967,:,:,:]\n",
        "label=label[0:179967,:]\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vmcM_-aKoeE"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import LearningRateScheduler\n",
        "def decay_schedule(epoch, lr):\n",
        "    if (epoch % 30 == 0) and (epoch != 0):\n",
        "        lr = lr * 0.5\n",
        "    if (epoch % 50 == 0) and (epoch != 0):\n",
        "        lr = lr * 0.2\n",
        "    return lr\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(decay_schedule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n726ncRhwSaW"
      },
      "outputs": [],
      "source": [
        " \n",
        "vae = VAE(encoder_cov, encoder_linear,decoder_cov,decoder_linear)\n",
        "\n",
        "vae.compile(optimizer=keras.optimizers.Adam(0.001),run_eagerly=True)\n",
        "\n",
        "if pou=='indicator':\n",
        " vae.fit(x_train,label,epochs=56, batch_size=batch_size,callbacks=[lr_scheduler])\n",
        "if pou=='smooth':\n",
        " A=A.astype('float32')\n",
        " vae.fit(x_train,epochs=56, batch_size=batch_size)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aOrnQu3m2Wt"
      },
      "source": [
        "# Evaluation \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UT9JQrNyMQ0E"
      },
      "outputs": [],
      "source": [
        "###generate image#######\n",
        "\n",
        "z_sample=[]\n",
        "for i in range(center):\n",
        "  z_sample.append(np.random.normal(size=(int(nprob[i]), latent_dim)))\n",
        "a=decoder_cov(decoder_linear(z_sample[0])[:,(0*intermediate_dim):((0+1)*intermediate_dim)])\n",
        "for i in range(1,center):\n",
        "  a=tf.concat([decoder_cov(decoder_linear(z_sample[i])[:,(i*intermediate_dim):((i+1)*intermediate_dim)]),a],axis=0)\n",
        "\n",
        "a=tf.random.shuffle(a) \n",
        " \n",
        "a=a.numpy() \n",
        "n = 8\n",
        "img_size = 64\n",
        "img_chns=3\n",
        "figure = np.zeros((img_size * n, img_size * n, img_chns))\n",
        "o=0\n",
        "for i in range(n):\n",
        "    for j in range(n):\n",
        "        x_decoded =a[o,:,:,:]\n",
        "        image_grid = vutils.make_grid(torch.Tensor(x_decoded),\n",
        "                              padding=2,\n",
        "                              normalize=True)\n",
        "        img = x_decoded.reshape(img_size, img_size, img_chns)\n",
        "        figure[i * img_size: (i + 1) * img_size,j * img_size: (j + 1) * img_size] = img\n",
        "        o=o+1\n",
        "         \n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(figure)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### Compute FID  and KID###\n",
        "\n",
        "##the codes are from the book ``Generative adversarial networks with python: deep learning generative models for image synthesis and image translation'' by Brownlee, Jason and the official implementation of the paper ``Demystifying MMD GANs'' by MikoÅ‚aj Binkowsk et.al."
      ],
      "metadata": {
        "id": "xCLcgGFiMOnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXuXT9LZf59T"
      },
      "outputs": [],
      "source": [
        " \n",
        "from numpy import cov\n",
        "from numpy import trace\n",
        "from numpy import iscomplexobj\n",
        "from numpy import asarray\n",
        " \n",
        "from scipy.linalg import sqrtm\n",
        " \n",
        "from keras.datasets.mnist import load_data\n",
        " \n",
        " \n",
        " \n",
        "# scale an array of images to a new size\n",
        "def scale_images(images, new_shape):\n",
        "\timages_list = list()\n",
        "\tfor image in images:\n",
        "\t\t# resize with nearest neighbor interpolation\n",
        "\t\tnew_image = resize(image, new_shape, 0)\n",
        "\t\t# store\n",
        "\t\timages_list.append(new_image)\n",
        "\treturn asarray(images_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_i-PQ6EVchiV"
      },
      "outputs": [],
      "source": [
        "\n",
        "from math import floor\n",
        "from numpy import ones\n",
        "from numpy import expand_dims\n",
        "from numpy import log\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import exp\n",
        "from numpy.random import shuffle\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from skimage.transform import resize\n",
        "from numpy import asarray\n",
        "import gc\n",
        "def calculate_fid_score(images,x_test1,n_split1=10, n_split2=10, eps=1E-16):\n",
        "    # load inception v3 model\n",
        "  model =InceptionV3(include_top=False, pooling='avg', input_shape=(299,299,3))\n",
        "    # enumerate splits of images/predictions\n",
        " \n",
        " \n",
        "  act1=[]\n",
        "  act2=[]\n",
        "  for i in range(n_split1):\n",
        "      n_part = floor(images.shape[0] / n_split1)\n",
        "      ix_start = i* n_part\n",
        "      ix_end =(i+1)* n_part\n",
        "      subset = images[ix_start:ix_end]\n",
        "     \n",
        "      subset = subset.astype('float32')\n",
        "    \n",
        "      \n",
        "      # scale images to the required size\n",
        "      subset = scale_images(subset, (299,299,3))\n",
        "      subset = preprocess_input(subset)\n",
        "      act1.append(model.predict(subset))\n",
        "      \n",
        "      del(subset)\n",
        "      gc.collect()\n",
        "      print(i)\n",
        "  for i in range(n_split2):\n",
        "      n_part = floor(x_test1.shape[0] / n_split2)\n",
        "      ix_start = i* n_part\n",
        "      ix_end =(i+1)* n_part \n",
        "      subset1 = x_test1[ix_start:ix_end]\n",
        "      subset1 = subset1.astype('float32')\n",
        "      subset1 = scale_images(subset1, (299,299,3))\n",
        "      subset1 = preprocess_input(subset1)\n",
        "      act2.append(model.predict(subset1))\n",
        "     \n",
        "      del(subset1)\n",
        "      gc.collect()\n",
        "      print(i)\n",
        "  # # calculate sum squared difference between means\n",
        "  act1=tf.concat(act1,axis=0)\n",
        "  act2=tf.concat(act2,axis=0)\n",
        "  act1=np.array(act1)\n",
        "  act2=np.array(act2)\n",
        "  mu1, sigma1 = act1.mean(axis=0), cov(act1, rowvar=False)\n",
        "  mu2, sigma2 = act2.mean(axis=0), cov(act2, rowvar=False)\n",
        "  ssdiff = np.sum((mu1 - mu2)**2.0)\n",
        "    # calculate sqrt of product between cov\n",
        "  covmean = sqrtm(sigma1.dot(sigma2))\n",
        "    # check and correct imaginary numbers from sqrt\n",
        "  if iscomplexobj(covmean):\n",
        "          covmean = covmean.real\n",
        "    # calculate score\n",
        "  fid = ssdiff + trace(sigma1 + sigma2 - 2.0 * covmean)\n",
        "  print(fid)\n",
        "      \n",
        "  return fid"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from __future__ import division, print_function\n",
        "\n",
        "import os.path, sys, tarfile\n",
        "import numpy as np\n",
        "from scipy import linalg\n",
        "from six.moves import range, urllib\n",
        "from sklearn.metrics.pairwise import polynomial_kernel\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "\n",
        "def polynomial_mmd(codes_g, codes_r, degree=3, gamma=None, coef0=1,\n",
        "                   var_at_m=None, ret_var=True):\n",
        "    # use  k(x, y) = (gamma <x, y> + coef0)^degree\n",
        "    # default gamma is 1 / dim\n",
        "    X = codes_g\n",
        "    Y = codes_r\n",
        "\n",
        "    K_XX = polynomial_kernel(X, degree=degree, gamma=gamma, coef0=coef0)\n",
        "    K_YY = polynomial_kernel(Y, degree=degree, gamma=gamma, coef0=coef0)\n",
        "    K_XY = polynomial_kernel(X, Y, degree=degree, gamma=gamma, coef0=coef0)\n",
        "\n",
        "    return _mmd2_and_variance(K_XX, K_XY, K_YY,\n",
        "                              var_at_m=var_at_m, ret_var=ret_var)\n",
        "\n",
        "\n",
        "def _sqn(arr):\n",
        "    flat = np.ravel(arr)\n",
        "    return flat.dot(flat)\n",
        "\n",
        "\n",
        "def _mmd2_and_variance(K_XX, K_XY, K_YY, unit_diagonal=False,\n",
        "                       mmd_est='unbiased', block_size=1024,\n",
        "                       var_at_m=None, ret_var=False):\n",
        "    m = K_XX.shape[0]\n",
        "    assert K_XX.shape == (m, m)\n",
        "    assert K_XY.shape == (m, m)\n",
        "    assert K_YY.shape == (m, m)\n",
        "    if var_at_m is None:\n",
        "        var_at_m = m\n",
        "\n",
        "    # Get the various sums of kernels that we'll use\n",
        "    # Kts drop the diagonal, but we don't need to compute them explicitly\n",
        "    if unit_diagonal:\n",
        "        diag_X = diag_Y = 1\n",
        "        sum_diag_X = sum_diag_Y = m\n",
        "        sum_diag2_X = sum_diag2_Y = m\n",
        "    else:\n",
        "        diag_X = np.diagonal(K_XX)\n",
        "        diag_Y = np.diagonal(K_YY)\n",
        "\n",
        "        sum_diag_X = diag_X.sum()\n",
        "        sum_diag_Y = diag_Y.sum()\n",
        "\n",
        "        sum_diag2_X = _sqn(diag_X)\n",
        "        sum_diag2_Y = _sqn(diag_Y)\n",
        "\n",
        "    Kt_XX_sums = K_XX.sum(axis=1) - diag_X\n",
        "    Kt_YY_sums = K_YY.sum(axis=1) - diag_Y\n",
        "    K_XY_sums_0 = K_XY.sum(axis=0)\n",
        "    K_XY_sums_1 = K_XY.sum(axis=1)\n",
        "\n",
        "    Kt_XX_sum = Kt_XX_sums.sum()\n",
        "    Kt_YY_sum = Kt_YY_sums.sum()\n",
        "    K_XY_sum = K_XY_sums_0.sum()\n",
        "\n",
        "    if mmd_est == 'biased':\n",
        "        mmd2 = ((Kt_XX_sum + sum_diag_X) / (m * m)\n",
        "                + (Kt_YY_sum + sum_diag_Y) / (m * m)\n",
        "                - 2 * K_XY_sum / (m * m))\n",
        "    else:\n",
        "        assert mmd_est in {'unbiased', 'u-statistic'}\n",
        "        mmd2 = (Kt_XX_sum + Kt_YY_sum) / (m * (m-1))\n",
        "        if mmd_est == 'unbiased':\n",
        "            mmd2 -= 2 * K_XY_sum / (m * m)\n",
        "        else:\n",
        "            mmd2 -= 2 * (K_XY_sum - np.trace(K_XY)) / (m * (m-1))\n",
        "\n",
        "    if not ret_var:\n",
        "        return mmd2\n",
        "\n",
        "    Kt_XX_2_sum = _sqn(K_XX) - sum_diag2_X\n",
        "    Kt_YY_2_sum = _sqn(K_YY) - sum_diag2_Y\n",
        "    K_XY_2_sum = _sqn(K_XY)\n",
        "\n",
        "    dot_XX_XY = Kt_XX_sums.dot(K_XY_sums_1)\n",
        "    dot_YY_YX = Kt_YY_sums.dot(K_XY_sums_0)\n",
        "\n",
        "    m1 = m - 1\n",
        "    m2 = m - 2\n",
        "    zeta1_est = (\n",
        "        1 / (m * m1 * m2) * (\n",
        "            _sqn(Kt_XX_sums) - Kt_XX_2_sum + _sqn(Kt_YY_sums) - Kt_YY_2_sum)\n",
        "        - 1 / (m * m1)**2 * (Kt_XX_sum**2 + Kt_YY_sum**2)\n",
        "        + 1 / (m * m * m1) * (\n",
        "            _sqn(K_XY_sums_1) + _sqn(K_XY_sums_0) - 2 * K_XY_2_sum)\n",
        "        - 2 / m**4 * K_XY_sum**2\n",
        "        - 2 / (m * m * m1) * (dot_XX_XY + dot_YY_YX)\n",
        "        + 2 / (m**3 * m1) * (Kt_XX_sum + Kt_YY_sum) * K_XY_sum\n",
        "    )\n",
        "    zeta2_est = (\n",
        "        1 / (m * m1) * (Kt_XX_2_sum + Kt_YY_2_sum)\n",
        "        - 1 / (m * m1)**2 * (Kt_XX_sum**2 + Kt_YY_sum**2)\n",
        "        + 2 / (m * m) * K_XY_2_sum\n",
        "        - 2 / m**4 * K_XY_sum**2\n",
        "        - 4 / (m * m * m1) * (dot_XX_XY + dot_YY_YX)\n",
        "        + 4 / (m**3 * m1) * (Kt_XX_sum + Kt_YY_sum) * K_XY_sum\n",
        "    )\n",
        "    var_est = (4 * (var_at_m - 2) / (var_at_m * (var_at_m - 1)) * zeta1_est\n",
        "               + 2 / (var_at_m * (var_at_m - 1)) * zeta2_est)\n",
        "\n",
        "    return mmd2, var_est\n"
      ],
      "metadata": {
        "id": "lDRsT6fQS3rC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0qQF7reEBX1"
      },
      "outputs": [],
      "source": [
        " \n",
        "def calculate_kid_score(images,x_test1,n_split1=10, n_split2=10, eps=1E-16):\n",
        "    # load inception v3 model\n",
        "  model =InceptionV3(include_top=False, pooling='avg', input_shape=(299,299,3))\n",
        "    # enumerate splits of images/predictions\n",
        "  #scores = list()\n",
        " \n",
        "  act1=[]\n",
        "  act2=[]\n",
        "  for i in range(n_split1):\n",
        "      n_part = floor(images.shape[0] / n_split1)\n",
        "      ix_start = i* n_part\n",
        "      ix_end =(i+1)* n_part\n",
        "      subset = images[ix_start:ix_end]\n",
        "     \n",
        "      subset = subset.astype('float32')\n",
        "    \n",
        "     \n",
        "      subset = scale_images(subset, (299,299,3))\n",
        "      subset = preprocess_input(subset)\n",
        "      act1.append(model.predict(subset))\n",
        "      \n",
        "      del(subset)\n",
        "      gc.collect()\n",
        "      print(i)\n",
        "  for i in range(n_split2):\n",
        "      n_part = floor(x_test1.shape[0] / n_split2)\n",
        "      ix_start = i* n_part\n",
        "      ix_end =(i+1)* n_part \n",
        "      subset1 = x_test1[ix_start:ix_end]\n",
        "      subset1 = subset1.astype('float32')\n",
        "      subset1 = scale_images(subset1, (299,299,3))\n",
        "      subset1 = preprocess_input(subset1)\n",
        "      act2.append(model.predict(subset1))\n",
        "     \n",
        "      del(subset1)\n",
        "      gc.collect()\n",
        "      print(i)\n",
        "   # calculate sum squared difference between means\n",
        "  act1=tf.concat(act1,axis=0)\n",
        "  act2=tf.concat(act2,axis=0)\n",
        "  fid =polynomial_mmd(act1,act2)\n",
        "      # store\n",
        "  print(fid)\n",
        "      \n",
        "  return fid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ONdf3zWvUCz"
      },
      "outputs": [],
      "source": [
        "z_sample=[]\n",
        "for i in range(center):\n",
        "  z_sample.append(np.random.normal(size=(int(np.round(nprob[i])), latent_dim)))\n",
        "\n",
        "a=decoder_cov(decoder_linear(z_sample[0])[:,(0*intermediate_dim):((0+1)*intermediate_dim)])\n",
        "for i in range(1,center):\n",
        "  a=tf.concat([decoder_cov(decoder_linear(z_sample[i])[:,(i*intermediate_dim):((i+1)*intermediate_dim)]),a],axis=0)\n",
        "\n",
        "for k in range(1,200):\n",
        "  z_sample=[]\n",
        "  for i in range(center):\n",
        "      z_sample.append(np.random.normal(size=(int(np.round(nprob[i])), latent_dim)))\n",
        "  for i in range(center):\n",
        "      a=tf.concat([decoder_cov(decoder_linear(z_sample[i])[:,(i*intermediate_dim):((i+1)*intermediate_dim)]),a],axis=0)\n",
        "\n",
        "image=np.reshape(np.array(a),(a.shape[0],64,64,3))*255\n",
        "x_test1=np.reshape(np.array(x_test[0:20000,:,:,:]),(x_test[0:20000,:,:,:].shape[0],64,64,3))*255\n",
        "calculate_kid_score(image,x_test1)\n",
        "#calculate_fid_score(image,x_test1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}